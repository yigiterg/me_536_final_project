{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageNet_KMEansClustering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iuyz0DAFvig8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential , Input\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Conv2D, UpSampling2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import binary_crossentropy \n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.layers.experimental.preprocessing import  Rescaling, Resizing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy.matlib\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from tensorflow.image import resize\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import normalize\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans , DBSCAN, AgglomerativeClustering\n",
        "from sklearn.preprocessing import normalize, StandardScaler\n",
        "from skimage.io import imread , imshow\n",
        "from scipy import spatial\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pickle \n",
        "import joblib"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXjaBTWFVWFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7becae24-1747-4854-bf01-e86da3b07887"
      },
      "source": [
        "!gdown --id 11FCdleqCwmHbFqVEKrbZy4QrQMhM2xQW ##Download animals.npy directly from google drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11FCdleqCwmHbFqVEKrbZy4QrQMhM2xQW\n",
            "To: /content/animals.npy\n",
            "693MB [00:02, 267MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otn2YZXxVZEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "998ec239-066d-461c-f0d5-a96364e22306"
      },
      "source": [
        " ##NOTE for instructors.  I directly loaded file from google drive during testing, but it is not\r\n",
        "##possible to share the 'animals.npy' from github since it was more than 600 mb therefore\r\n",
        "## fetching file directly from google drive more easy for you to download to this environment\r\n",
        "dataset = np.load('animals.npy')  ##Images dataset represented as image tensor\r\n",
        "print(dataset.shape)\r\n",
        "print(\"Done!\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14092, 128, 128, 3)\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPuElcznVR4F"
      },
      "source": [
        "!gdown --id 18CQ8_jInPhWNaiIPEBWPHx35prT9xqlw ## Download img_array directly from google-drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg7oTeMJWPMu"
      },
      "source": [
        "img_array = np.load('224_224img.npy') #Rows have the feature vector of 10000 images from dataset.\r\n",
        "img_array.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhj7el-ZD4A3"
      },
      "source": [
        "## MobileNet model trained on ImageNet. Directly download from TensorFlow Hub\r\n",
        "m = tf.keras.Sequential([\r\n",
        "    hub.KerasLayer(\"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\r\n",
        "                   trainable=False),  # Can be True, see below.\r\n",
        "])\r\n",
        "m.build([None, 224, 224, 3])  # Batch input shape."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sbIqwqG6LGl"
      },
      "source": [
        "## Scale the input data before doing different PCA operations. \n",
        "scaled = StandardScaler().fit(img_array)\n",
        "scaled_array = scaled.transform(img_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFrDXn9r6iSC"
      },
      "source": [
        "pca = PCA(n_components = 30) ##Reduce feature space dimension and cluster its data so that we get a labeling of classes.\n",
        "pca.fit(scaled_array)\n",
        "print(f'Total variance of reduced dimension representation is = {round(np.sum(pca.explained_variance_ratio_)*100,3)}%') ##Total variance \n",
        "compressed_vector = pca.transform(scaled_array)\n",
        "print(f'New shape of array = {compressed_vector.shape}')\n",
        "kmeans = KMeans(n_clusters= 5 ,n_jobs=-1, random_state=22, max_iter= 500)\n",
        "kmeans.fit(compressed_vector)\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvhWGszTr88c"
      },
      "source": [
        "##To visualize the data, reduce the dimension of input to 3. Visualize below\r\n",
        "pca_3d = PCA(n_components = 3)\r\n",
        "pca_3d.fit(scaled_array)\r\n",
        "feature_vector = pca_3d.transform(scaled_array)\r\n",
        "kmeans_10k = KMeans(n_clusters= 5 ,n_jobs=-1, random_state=22, max_iter= 500)\r\n",
        "kmeans_10k.fit(feature_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf7SnRetrWOZ"
      },
      "source": [
        "fig = plt.figure(figsize=(10, 6))\r\n",
        "# Create 3D container of 10.000 Data Point\r\n",
        "ax = plt.axes(projection = '3d')\r\n",
        "# Visualize 3D scatter plot\r\n",
        "v = np.zeros((10000,4)) ##Visualize 10000 images in the image array in 3D feature space.\r\n",
        "v[:,:3] = feature_vector\r\n",
        "v[:,3] = kmeans_10k.labels_\r\n",
        "df = pd.DataFrame(v, columns=['Feature1', 'Feature2','Feature3', \"Cluster\"])\r\n",
        "ax.scatter(feature_vector[:,0],feature_vector[:,1],feature_vector[:,2], marker=\"o\", c=df[\"Cluster\"], s=40, cmap=\"RdBu\")\r\n",
        "# Give labels\r\n",
        "ax.set_xlabel('x')\r\n",
        "ax.set_ylabel('y')\r\n",
        "ax.set_zlabel('z')\r\n",
        "# Save figure\r\n",
        "plt.savefig('3d_scatter.png', dpi = 400);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNYoUpkj6tBT"
      },
      "source": [
        "##Soft K-Means Clustering Function\n",
        "def soft_clustering_weights(data, cluster_centres, **kwargs):\n",
        "    \n",
        "    \"\"\"\n",
        "    Function to calculate the weights from soft k-means\n",
        "    data: Array of data. shape = N x F, for N data points and F Features\n",
        "    cluster_centres: Array of cluster centres. shape = Nc x F, for Nc number of clusters. Input kmeans.cluster_centres_ directly.\n",
        "    param: m - keyword argument, fuzziness of the clustering. Default 2\n",
        "    \"\"\"\n",
        "    \n",
        "    # Fuzziness parameter m>=1. Where m=1 => hard segmentation\n",
        "    m = 2\n",
        "    if 'm' in kwargs:\n",
        "        m = kwargs['m']\n",
        "    \n",
        "    Nclusters = cluster_centres.shape[0]\n",
        "    Ndp = data.shape[0]\n",
        "    Nfeatures = data.shape[1]\n",
        "\n",
        "    # Get distances from the cluster centres for each data point and each cluster\n",
        "    EuclidDist = np.zeros((Ndp, Nclusters))\n",
        "    for i in range(Nclusters):\n",
        "        EuclidDist[:,i] = np.sum((data-numpy.matlib.repmat(cluster_centres[i], Ndp, 1))**2,axis=1)\n",
        "\n",
        "    \n",
        "    # Denominator of the weight from wikipedia:\n",
        "    invWeight = EuclidDist**(2/(m-1))*numpy.matlib.repmat(np.sum((1./EuclidDist)**(2/(m-1)),axis=1).reshape(-1,1),1,Nclusters)\n",
        "    Weight = 1./invWeight\n",
        "    \n",
        "    return Weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXFEeH_z6vHY"
      },
      "source": [
        "weights = soft_clustering_weights(compressed_vector,kmeans.cluster_centers_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjLabUn53KNa"
      },
      "source": [
        "animal_dict = {3 : \"Butterfly\", 4 : \"Horse\", 1 : \"Elephant\", 2 : \"Chicken\", 0 : \"Dog\"} ##This numbers may change after  clustering! Since label of each class changes!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5bztCMb61nx"
      },
      "source": [
        "##Test Clustering Result. Change example with 'data' variable.It goes up to 9999 if you did upload image tensor (dataset variable). Classes probabilities depends on the labeling. They may change after each clustering operation.\n",
        "data = 1568\n",
        "plt.imshow(dataset[data])\n",
        "print(f'Chicken Prob. = {round(weights[data][2],3)} \\nButterfly Prob. = {round(weights[data][3],3)} \\nDog Prob. = {round(weights[data][0],3)} \\nElephant Prob. = {round(weights[data][1],3)} \\nHorse Prob. = {round(weights[data][4],3)}\\n')\n",
        "print(animal_dict[kmeans.labels_[data]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLXIAqwamt_o"
      },
      "source": [
        "#Split all classes to use equal number of images from each classes \n",
        "image_index = []\n",
        "dog_img = np.array(img_array[8])\n",
        "dog_count = 1\n",
        "chicken_img = np.array(img_array[3])\n",
        "chicken_count = 1\n",
        "elephant_img = np.array(img_array[0])\n",
        "elephant_count = 1\n",
        "horse_img = np.array(img_array[1])\n",
        "horse_count = 1\n",
        "butterfly_img = np.array(img_array[4])\n",
        "butterfly_count = 1 \n",
        "for i in range(0,img_array.shape[0]) :\n",
        "  if kmeans.labels_[i] == 2 and dog_count <= 1000 : ## It means it is dog\n",
        "    dog_count += 1\n",
        "    dog_img = np.vstack((dog_img, img_array[i]))\n",
        "    image_index.append(i)\n",
        "  if kmeans.labels_[i] == 0 and chicken_count <= 1000: ## It means it is chicken\n",
        "    chicken_img = np.vstack((chicken_img,img_array[i]))\n",
        "    chicken_count += 1 \n",
        "    image_index.append(i)\n",
        "  if kmeans.labels_[i] == 1 and elephant_count <= 1000: ## Elephant\n",
        "    elephant_img = np.vstack((elephant_img, img_array[i]))\n",
        "    elephant_count += 1\n",
        "    image_index.append(i)\n",
        "  if kmeans.labels_[i] == 3  and horse_count <= 1000: ## horse :\n",
        "    horse_img = np.vstack((horse_img, img_array[i]))\n",
        "    horse_count += 1\n",
        "    image_index.append(i)\n",
        "  if kmeans.labels_[i] == 4 and butterfly_count <= 1000: ## butterfly\n",
        "    butterfly_img = np.vstack((butterfly_img, img_array[i]))\n",
        "    butterfly_count += 1\n",
        "    image_index.append(i)\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62AlY8DoCfeE"
      },
      "source": [
        "# Concatenate animal feature vectors and shuffle them.\r\n",
        "final_arr = np.concatenate((dog_img,chicken_img,elephant_img,horse_img,butterfly_img))\r\n",
        "np.random.shuffle(final_arr) # Shuffle the input\r\n",
        "print(final_arr.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7Lc308fswuE"
      },
      "source": [
        "pca_5000 = PCA(n_components=3) ## Visualize the animal feature vectors in 3D space.\r\n",
        "pca_5000.fit(final_arr)\r\n",
        "transform_fina = pca_5000.transform(final_arr)\r\n",
        "kmeans_5000 = KMeans(n_clusters= 5 ,n_jobs=-1, random_state=22, max_iter= 500)\r\n",
        "kmeans_5000.fit(transform_fina)\r\n",
        "fig = plt.figure(figsize=(10, 6))\r\n",
        "# Create 3D container of \r\n",
        "ax = plt.axes(projection = '3d')\r\n",
        "# Visualize 3D scatter plot\r\n",
        "v = np.zeros((final_arr.shape[0],4))\r\n",
        "v[:,:3] = transform_fina\r\n",
        "v[:,3] = kmeans_5000.labels_\r\n",
        "df = pd.DataFrame(v, columns=['Feature1', 'Feature2','Feature3', \"Cluster\"])\r\n",
        "ax.scatter(transform_fina[:,0],transform_fina[:,1],transform_fina[:,2], marker=\"o\", c=df[\"Cluster\"], s=40, cmap=\"RdBu\")\r\n",
        "# Give labels\r\n",
        "ax.set_xlabel('x')\r\n",
        "ax.set_ylabel('y')\r\n",
        "ax.set_zlabel('z')\r\n",
        "# Save figure\r\n",
        "plt.savefig('3d_scatter.png', dpi = 400);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iZpOMXbChTC"
      },
      "source": [
        "#Reduce feature vector dimension. Cluster them use their labels in the classifier chain.\r\n",
        "scaled = StandardScaler().fit(final_arr)\r\n",
        "scaled_final = scaled.transform(final_arr)\r\n",
        "pca_final = PCA(n_components = 50)\r\n",
        "pca_final.fit(scaled_final)\r\n",
        "print(np.sum(pca_final.explained_variance_ratio_))\r\n",
        "compressed_final = pca_final.transform(scaled_final)\r\n",
        "print(compressed_final.shape)\r\n",
        "kmeans_final = KMeans(n_clusters= 5 ,n_jobs=-1, random_state=22, max_iter= 500)\r\n",
        "kmeans_final.fit(compressed_final)\r\n",
        "## ONE HOT ENCODING OF LABELS\r\n",
        "a = np.array(kmeans_final.labels_)\r\n",
        "labels = np.zeros((a.size, a.max()+1))\r\n",
        "labels[np.arange(a.size),a] = 1\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XaOI1xEWoW3"
      },
      "source": [
        "##Download classifier chain from google drive\r\n",
        "!gdown --id 1-8jbTv79KtKVNoOozi2oW-V9zsPvR6Wr\r\n",
        "chain = joblib.load('chain.pkl') ##Optimized classifier chain model."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8BSjne66Gcx"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(normalize(compressed_final), labels, random_state=0, test_size = 0.1)\r\n",
        "a = chain.predict_proba(X_test)\r\n",
        "data = 1561 ##Change this to see a different example \r\n",
        "feat_res_1 = normalize(pca_final.transform(m.predict(resize(dataset[data],size=[224,224])[np.newaxis,...])))\r\n",
        "plt.imshow(dataset[data])\r\n",
        "print(chain.predict_proba(feat_res_1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abnPWqCHjdfc"
      },
      "source": [
        "Difference between two lines below, is the 'order' input which, changes  the ordering of the classes in the chain. I found the optimal ordering as Dog-Horse-Elephant-Chicken-Butterfly, but there are methods to find this optimal order automatically. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yooztP0FF-0C"
      },
      "source": [
        "##Order is random! Performance on unseen data differs by a lot!\r\n",
        "\r\n",
        "#X_train, X_test, Y_train, Y_test = train_test_split(normalize(compressed_final), labels, random_state=0, test_size = 0.1)\r\n",
        "#base_lr = LogisticRegression(solver='lbfgs', random_state=1)\r\n",
        "#chain = ClassifierChain(base_lr, order='random', random_state=1)\r\n",
        "#chain.fit(X_train, Y_train).predict(X_test)\r\n",
        "#a = chain.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3JKNNbfimD-"
      },
      "source": [
        "##Order is given to the function. Dog-Horse-Elephant-Chicken-Butterfly (Order of the Chain) Max Performance! Use ready to use model. \r\n",
        "#Because labeling changes after clustering above. Outputs may not match otherwise.\r\n",
        "\r\n",
        "#X_train, X_test, Y_train, Y_test = train_test_split(normalize(compressed_final), labels, random_state=0, test_size = 0.1)\r\n",
        "#base_lr = LogisticRegression(solver='lbfgs', random_state=1)\r\n",
        "#chain = ClassifierChain(base_lr, order=[4,3, 2,0,1], random_state=1)\r\n",
        "#chain.fit(X_train, Y_train).predict(X_test)\r\n",
        "#a = chain.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBqKZTfbaqWp"
      },
      "source": [
        "## This Function is for the performance change of the chain when the ordering differs.\r\n",
        "## Note that these results are on the trained class. Like I said, unseen data\r\n",
        "##Performance completely different from the graph. You can see it in the report.g\r\n",
        "base_lr = LogisticRegression()\r\n",
        "ovr = OneVsRestClassifier(base_lr)\r\n",
        "ovr.fit(X_train, Y_train)\r\n",
        "Y_pred_ovr = ovr.predict(X_test)\r\n",
        "ovr_jaccard_score = jaccard_score(Y_test, Y_pred_ovr, average='samples')\r\n",
        "# Fit an ensemble of logistic regression classifier chains and take the\r\n",
        "# take the average prediction of all the chains.\r\n",
        "chains = [ClassifierChain(base_lr, order='random', random_state=i)\r\n",
        "          for i in range(10)]\r\n",
        "for each_chain in chains:\r\n",
        "    each_chain.fit(X_train, Y_train)\r\n",
        "Y_pred_chains = np.array([each_chain.predict(X_test) for chain in\r\n",
        "                          chains])\r\n",
        "chain_jaccard_scores = [jaccard_score(Y_test, Y_pred_chain >= .5,\r\n",
        "                                      average='samples')\r\n",
        "                        for Y_pred_chain in Y_pred_chains]\r\n",
        "\r\n",
        "Y_pred_ensemble = Y_pred_chains.mean(axis=0)\r\n",
        "ensemble_jaccard_score = jaccard_score(Y_test,\r\n",
        "                                       Y_pred_ensemble >= .5,\r\n",
        "                                       average='samples')\r\n",
        "\r\n",
        "model_scores = [ovr_jaccard_score] + chain_jaccard_scores\r\n",
        "model_scores.append(ensemble_jaccard_score)\r\n",
        "\r\n",
        "model_names = ('Independent',\r\n",
        "               'Chain 1',\r\n",
        "               'Chain 2',\r\n",
        "               'Chain 3',\r\n",
        "               'Chain 4',\r\n",
        "               'Chain 5',\r\n",
        "               'Chain 6',\r\n",
        "               'Chain 7',\r\n",
        "               'Chain 8',\r\n",
        "               'Chain 9',\r\n",
        "               'Chain 10',\r\n",
        "               'Ensemble')\r\n",
        "\r\n",
        "x_pos = np.arange(len(model_names))\r\n",
        "\r\n",
        "# Plot the Jaccard similarity scores for the independent model, each of the\r\n",
        "# chains, and the ensemble (note that the vertical axis on this plot does\r\n",
        "# not begin at 0).\r\n",
        "\r\n",
        "fig, ax = plt.subplots(figsize=(7, 4))\r\n",
        "ax.grid(True)\r\n",
        "ax.set_title('Classifier Chain Ensemble Performance Comparison')\r\n",
        "ax.set_xticks(x_pos)\r\n",
        "ax.set_xticklabels(model_names, rotation='vertical')\r\n",
        "ax.set_ylabel('Jaccard Similarity Score')\r\n",
        "ax.set_ylim([min(model_scores) * .9, max(model_scores) * 1.1])\r\n",
        "colors = ['r'] + ['b'] * len(chain_jaccard_scores) + ['g']\r\n",
        "ax.bar(x_pos, model_scores, alpha=0.5, color=colors)\r\n",
        "plt.tight_layout()\r\n",
        "plt.show()\r\n",
        "\r\n",
        "##Classifier Chain Randomly Select \"starting class\" and in other words \"ordering\" of classes, therefore\r\n",
        "##Their order also changes the performance of the classifier chain.\r\n",
        "##In this example though there is not much difference between different class order!\r\n",
        "##BUT, when system is introduced with \"unseen\" data, the responses will differ by a lot!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR3WqBnn27DU"
      },
      "source": [
        " ##Percentage on the test set that alert the system where they should not have! (out of 499)\r\n",
        "threshold = 0.80\r\n",
        "repetitive = 0\r\n",
        "for results in a : \r\n",
        "  res = np.amax(results)\r\n",
        "  if res < threshold :\r\n",
        "    repetitive += 1\r\n",
        "print(f'{round((100 - repetitive/499 * 100),2)}% accuracy on the test set')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPpru-uuevEs"
      },
      "source": [
        "!gdown --id 1UaoB4Ajna-FW3YtnrXcQByi464DiaDr3\r\n",
        "!!unzip 'Test_Images-zipped.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHW8n7kaKEDc"
      },
      "source": [
        "### Test on Specific Animal Species.\r\n",
        "\r\n",
        "data = []\r\n",
        "drive_path = 'Test_Images/Random Animal Images/'\r\n",
        "net_sum = 0\r\n",
        "file_number = 0\r\n",
        "under_threshold = 0\r\n",
        "above_threshold = 0\r\n",
        "for filename in os.listdir(drive_path):\r\n",
        "    if filename.endswith(\"jpg\"): \r\n",
        "        file_number += 1\r\n",
        "        # Your code comes here such as \r\n",
        "        feat_res_1 = normalize(pca_final.transform(m.predict(resize((imread(drive_path+filename)/255.),[224,224])[np.newaxis,...])))\r\n",
        "        probabilities = chain.predict_proba(feat_res_1)\r\n",
        "        res = np.amax(probabilities)\r\n",
        "        net_sum += res\r\n",
        "        #print(res)\r\n",
        "        if res < threshold :\r\n",
        "          under_threshold += 1\r\n",
        "        else :\r\n",
        "          above_threshold += 1\r\n",
        "print(f' System detected new species with accuracy of {100 * (under_threshold/file_number)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiYuhMhu2BrG"
      },
      "source": [
        "### Test on whole test images dataset taken from google images.\r\n",
        "\r\n",
        "data = []\r\n",
        "drive_path = 'Test_Images/'\r\n",
        "net_sum = 0\r\n",
        "file_number = 0\r\n",
        "under_threshold = 0\r\n",
        "above_threshold = 0\r\n",
        "for filename in os.listdir(drive_path):\r\n",
        "  for filename_1 in os.listdir(drive_path+filename) :\r\n",
        "    if filename_1.endswith(\"jpg\"): \r\n",
        "        file_number += 1\r\n",
        "        feat_res_1 = normalize(pca_final.transform(m.predict(resize((imread(drive_path+filename+\"/\"+filename_1)/255.),[224,224])[np.newaxis,...])))\r\n",
        "        probabilities = chain.predict_proba(feat_res_1)\r\n",
        "        res = np.amax(probabilities)\r\n",
        "        net_sum += res\r\n",
        "        if res < threshold :\r\n",
        "          under_threshold += 1\r\n",
        "        else :\r\n",
        "          above_threshold += 1\r\n",
        "print(f' System detected new species with accuracy of {100 * (under_threshold/file_number)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C00M9J1zxQ_o"
      },
      "source": [
        "animal_dict = {3 : \"Dog\", 4 : \"Horse\", 2 : \"Elephant\", 0 : \"Chicken\", 1 : \"Butterfly\"} ##This is the label dictionary for ready 2 use scikit model I trained "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDSJO9RBGJlg"
      },
      "source": [
        "image_url = 'https://www.diyadinnet.com/d/ruya/ruyada-at-gormek-nedir-siyah-beyaz-ata-binmek-ne-anlama-gelir-3280.jpg' #Test Horse Class\r\n",
        "#image_url = 'https://media.tacdn.com/media/attractions-splice-spp-674x446/09/19/7c/5f.jpg'  #Test Elephant Class\r\n",
        "#image_url = 'https://media.tacdn.com/media/attractions-splice-spp-674x446/06/e4/c9/d4.jpg'  #Test Butterfly Class\r\n",
        "#image_url = 'https://morningchores.com/wp-content/uploads/2018/12/Everything-You-Should-Consider-for-Solid-Chicken-Health-Management-FI.jpg' #Test Chicken class\r\n",
        "#image_url = 'https://upload.wikimedia.org/wikipedia/commons/2/23/Canis_lupus.jpg' #Test Dog Class or Unseen Species.\r\n",
        "feat_res = normalize(pca_final.transform(m.predict(resize((imread(image_url)/255.),[224,224])[np.newaxis,...])))\r\n",
        "result = (chain.predict_proba(feat_res))\r\n",
        "max_perc = np.amax(result)\r\n",
        "max_index = np.argmax(result)\r\n",
        "##Dogs have to be handled separately. Because there are a lot of dog species out there and\r\n",
        "## Dataset does not contain much of these species most of them labrador or golden.\r\n",
        "## I found out after the checking the system with couple of different dog species. Works quite well.\r\n",
        "if max_perc < threshold and max_index != 3  :\r\n",
        " print(f\"Max similarity percentage is {round(max_perc * 100,2)}% and system says it can be a {animal_dict[max_index]}. Percentage is not high enough though. This might be a new species. Let me Zero-Shot it...\")\r\n",
        " imshow(image_url)\r\n",
        "\r\n",
        "elif max_perc > 60 and max_index == 3 : ## 3 is dog index.\r\n",
        "  print(f\"I know this species! It is a {animal_dict[max_index]} with probability of {round(max_perc * 100,2)}%\")\r\n",
        "  imshow(image_url)\r\n",
        "else :\r\n",
        "  print(f\"I know this species! It is a {animal_dict[max_index]} with probability of {round(max_perc * 100,2)}%\")\r\n",
        "  imshow(image_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31H-cvXAGS75"
      },
      "source": [
        "##BELOW IS RELATED ZERO SHOT  / Speculation Part"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_pBaUwb_sth"
      },
      "source": [
        "# !brew install wget ##Word2Vec Do not try to use and extract these here. Session will crush due to excessive use of RAM!\n",
        "\n",
        "# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSoF-Sv8YJX4"
      },
      "source": [
        "!gdown --id 14za0w3HRSb1IQRq6sUzbLYHLJcaMX3Mx ##Zebra.npy\r\n",
        "!gdown --id 1WqKZfikA3t5FyIlkZ_rU3gH4LQEvzMsl ##Wolf.npy\r\n",
        "!gdown --id 1wlDoWtSls4aPRiT2t3eB55VgZkUCuH9z ##Spider.npy\r\n",
        "!gdown --id 1WxII7q0XAabcoUHjscEx_sstQKUuuRT7 ##Human.npy\r\n",
        "!gdown --id 1EmJZ0nsgosEKpHncl8xPSSlV-IG7ZPUU ##Horse.npy\r\n",
        "!gdown --id 1K9FPPXj-Tw_tzGNrOAkBfF1kI7OEOcOX ##Giraffe.npy\r\n",
        "!gdown --id 1N4PTE5AGi24NxgkBg--I00r7nNzc-W4W ##Fox.npy\r\n",
        "!gdown --id 1Ue6hzoygIW_W7k71pMtXTPsbZR9cxvzX ##Elephant.npy\r\n",
        "!gdown --id 1z0Ng0LTfhZitW3dsjM8-Whgq0YFJ-BaA ##Dog.npy\r\n",
        "!gdown --id 1qhPZCmZDrdQwbpdKYLIlKysngzFr_PSJ ##Chicken.npy\r\n",
        "!gdown --id 1FbqtVRYm-aDQ4Sy2RCaxuBDeydWkAVY6 ##Cat.npy\r\n",
        "!gdown --id 1wIBciukjDb8yDoPFbgWs5VZrF7ehRh1I ##butterfly.npy\r\n",
        "!gdown --id 10krVBYq_gppAmKxEK7kGLLQ_k4nvDufo ##Bear.npy\r\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbabXoEQZWUo"
      },
      "source": [
        "##Zero-Shot and Training Class Word2Vec Representation. Each of them is a (300,) vector\r\n",
        "##It is almost impossible with only having 12 gb ram and upload whole word2vec in this environments.\r\n",
        "##Therefore i downloaded the word2vec in my local computer and just got what I needed for this model.\r\n",
        "\r\n",
        "'''\r\n",
        "ZERO-SHOT CLASSES :\r\n",
        "BEAR ZEBRA GIRAFFE SPIDER HUMAN FOX WOLF CAT\r\n",
        "\r\n",
        "TRAINING CLASSES :\r\n",
        "DOG HORSE ELEPHANT BUTTERFULY CHICKEN\r\n",
        "'''\r\n",
        "\r\n",
        "dog = np.load('dog.npy') \r\n",
        "horse = np.load('horse.npy')\r\n",
        "elephant = np.load('elephant.npy')\r\n",
        "butterfly =np.load('butterfly.npy')\r\n",
        "chicken = np.load('chicken.npy')\r\n",
        "bear = np.load('bear.npy')\r\n",
        "zebra = np.load('zebra.npy')\r\n",
        "giraffe = np.load('giraffe.npy')\r\n",
        "spider = np.load('spider.npy')\r\n",
        "human = np.load('human.npy')\r\n",
        "fox = np.load('fox.npy')\r\n",
        "wolf = np.load('wolf.npy')\r\n",
        "cat = np.load('cat.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsSU35tIaeu-"
      },
      "source": [
        "pca_clust = PCA(n_components = 30)\r\n",
        "pca_clust.fit(scaled_array)\r\n",
        "print(np.sum(pca_clust.explained_variance_ratio_))\r\n",
        "compressed_zsl = pca_clust.transform(scaled_array)\r\n",
        "print(compressed_zsl.shape)\r\n",
        "#kmeans_zsl = KMeans(n_clusters= 5 ,n_jobs=-1, random_state=22, max_iter= 500)\r\n",
        "#kmeans_zsl.fit(compressed_zsl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3mLVwLWZ7bn"
      },
      "source": [
        "!gdown --id 1-OP1f9mWWpiYEJ4D6q4EhabomRPLOwxa ##Download clustering result from drive ('kmeans_save.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvI72IWya-Rt"
      },
      "source": [
        "load_kmeans = pickle.load(open(\"kmeans_save.pkl\", \"rb\")) ## Load clustering result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnKSetvMbMcH"
      },
      "source": [
        "animal_dict = {4 : \"Dog\", 3 : \"Horse\", 0 : \"Elephant\", 2 : \"Chicken\", 1 : \"Butterfly\"} ##This is the label dictionary for ready 2 use scikit model I trained "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw0wuSygaFCQ"
      },
      "source": [
        "data = 23\r\n",
        "plt.imshow(dataset[data])\r\n",
        "print(f'Chicken Prob. = {round(weights[data][2],3)} \\nButterfly Prob. = {round(weights[data][1],3)} \\nDog Prob. = {round(weights[data][4],3)} \\nElephant Prob. = {round(weights[data][0],3)} \\nHorse Prob. = {round(weights[data][3],3)}\\n')\r\n",
        "print(animal_dict[load_kmeans.labels_[data]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq49h3jSxkIX"
      },
      "source": [
        "semantic_arr = np.array(elephant) ##First image was an elephant. To initialize semantic array, i directly used elephant semantic vector.\r\n",
        "##Create Semantic Array to Map onto. For zero-shot purposes. \r\n",
        "for i in range(1,kmeans.labels_.shape[0]) :\r\n",
        "  if kmeans.labels_[i] == 0 : ##It means it is elephant\r\n",
        "    semantic_arr = np.vstack((semantic_arr, elephant))\r\n",
        "  elif kmeans.labels_[i] == 1 : ##It means it is butterfly\r\n",
        "    semantic_arr = np.vstack((semantic_arr, butterfly))\r\n",
        "  elif kmeans.labels_[i] == 2 : ## It means it is chicken\r\n",
        "    semantic_arr = np.vstack((semantic_arr, chicken))\r\n",
        "  elif kmeans.labels_[i] == 3 : ##It means it is horse\r\n",
        "    semantic_arr = np.vstack((semantic_arr, horse))\r\n",
        "  elif kmeans.labels_[i] == 4 : ##It means it is dog\r\n",
        "    semantic_arr = np.vstack((semantic_arr, dog))\r\n",
        "print(semantic_arr.shape)\r\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh8kOBVQJgLK"
      },
      "source": [
        "def  build_model():\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Dense(2048, input_shape=(300,), activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dense(1024, activation='relu'))\r\n",
        "    model.add(Dense(512, activation='relu'))\r\n",
        "    model.add(Dense(300, activation='relu'))\r\n",
        "#    model.add(Dense(NUM_CLASS, activation='softmax', trainable=False, kernel_initializer=custom_kernel_init))\r\n",
        "\r\n",
        "    print(\"-> model building is completed.\")\r\n",
        "    return model\r\n",
        "def early_stopping() :\r\n",
        "  return EarlyStopping(\r\n",
        "      monitor = 'val_accuracy',\r\n",
        "      patience = 20\r\n",
        "  )\r\n",
        "def get_checkpoint(): \r\n",
        "  return ModelCheckpoint(\r\n",
        "    '/content/drive/MyDrive/Me536_Final_Project/Zero Shot Learning/zsl_best_corrected_semantic_array', monitor='val_accuracy', verbose=0, save_best_only=True,\r\n",
        "    save_weights_only=False, mode='auto', save_freq=\"epoch\",\r\n",
        "    options=None\r\n",
        ")\r\n",
        "##Training parameters of Zero-shot model\r\n",
        "\r\n",
        "\r\n",
        "#adam = Adam(lr=5e-5)\r\n",
        "#normalized_compressed = normalize(compressed_vector)\r\n",
        "#zsl_model = build_model()\r\n",
        "#zsl_model.compile(loss      = 'mean_squared_error',\r\n",
        "#                  optimizer = adam,\r\n",
        "#                  metrics   = ['accuracy'])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1piivschaLsR"
      },
      "source": [
        "!gdown --id 1GqgIs4KdjErBW7QPSMUhDs1qcmRnk69s ##Download zero-shot model from drive\r\n",
        "!unzip 'ZSL_Model_Another_zipped.zip' ##Unzip \r\n",
        "zsl_model = load_model('ZSL_Model_Another') ##Load the already trained zero-shot model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhS9agMFcp-w"
      },
      "source": [
        "zsl_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAaAcmeL6_38"
      },
      "source": [
        "class_dict = {\"dog\" : dog,\r\n",
        "     \"elephant\" :elephant,\r\n",
        "     \"horse\" : horse, \r\n",
        "     \"chicken\" : chicken, \r\n",
        "     \"butterfly\" : butterfly, \r\n",
        "     \"bear\" : bear,\r\n",
        "     \"zebra\" : zebra, \r\n",
        "     \"giraffe\" : giraffe, \r\n",
        "     \"spider\" : spider, \r\n",
        "      \"human\" : human, \r\n",
        "      \"fox\" : fox, \r\n",
        "      \"wolf\": wolf,\r\n",
        "      \"cat\" : cat }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY7px1gb7oZy"
      },
      "source": [
        "pca_zsl = PCA(n_components = 300) ##Reduce the dimension of input to the trained model input size.\r\n",
        "pca_zsl.fit(scaled_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4cuaSKlZuVs"
      },
      "source": [
        "path = 'https://icatcare.org/app/uploads/2018/06/Layer-1704-1920x840.jpg' ##Image path\r\n",
        "feat_res_1 = normalize(pca_zsl.transform((m.predict(resize((imread(path)/255.),[224,224])[np.newaxis,...])))) #Resize, get the feature vector from the trained model, reduce its dimension and normalize the input\r\n",
        "img_res_1 = zsl_model.predict(feat_res_1) #Get the prediction from the Zero-shot model\r\n",
        "img_res_1 = img_res_1.reshape(300,)\r\n",
        "distance_list_another = list()\r\n",
        "for word in class_dict :\r\n",
        "  x = class_dict[word]\r\n",
        "  dist = np.linalg.norm(x - img_res_1)\r\n",
        "  distance_list_another.append([dist, word])\r\n",
        "distance_list_another = sorted(distance_list_another, key = lambda x : x[0])\r\n",
        "print(\"Top 5 predictions are :\")\r\n",
        "for i in range(0,5) : ##Top 3 Class\r\n",
        "  print(f'{i+1} {distance_list_another[i][1]}')\r\n",
        "plt.imshow(imread(path))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}